\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{filecontents}
\usepackage{tikz}
\usetikzlibrary{calc,positioning}
\usepackage[ruled,vlined]{algorithm2e}

\SetKwProg{Fn}{Function}{ is}{}
\newcommand{\SetAlgoStyle}{
	\SetAlgoNoLine
	\SetAlgoNoEnd
	\DontPrintSemicolon
}

\newcommand{\fig}[2]{
	\begin{figure}[!htb]
		\center{\includegraphics[width=0.75\textwidth,keepaspectratio]{res/#1}}
		\caption{\label{fig:caption} #2}
	\end{figure}
}

\begin{filecontents}{\jobname.bib}
@misc{1301.3781,
	author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	title = {Efficient Estimation of Word Representations in Vector Space},
	year = {2013},
	eprint = {arXiv:1301.3781},
	note = {\url{https://arxiv.org/abs/1301.3781}}
},
@misc{1411.2738,
	author = {Xin Rong},
	title = {word2vec Parameter Learning Explained},
	year = {2014},
	eprint = {arXiv:1411.2738},
	note = {\url{https://arxiv.org/abs/1411.2738}}
},
@misc{1704.03956,
	author = {Nobuhiro Kaji and Hayato Kobayashi},
	title = {Incremental Skip-gram Model with Negative Sampling},
	year = {2017},
	eprint = {arXiv:1704.03956},
	note = {\url{https://arxiv.org/abs/1704.03956}}
},
@misc{1206.6426,
	author = {Andriy Mnih and Yee Whye Teh},
	title = {A Fast and Simple Algorithm for Training Neural Probabilistic Language Models},
	year = {2012},
	eprint = {arXiv:1206.6426},
	note = {\url{https://arxiv.org/abs/1206.6426}}
},
@misc{1512.05287,
	author = {Yarin Gal and Zoubin Ghahramani},
	title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
	year = {2015},
	eprint = {arXiv:1512.05287},
	note = {\url{https://arxiv.org/abs/1512.05287}}
}
\end{filecontents}

\title{SGNS word embedder optimization}
\author{Lazar JeliÄ‡}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

This note provides new insight on time and memory optimization techniques for
skip-gram word embedding neural network with autoencoder architecture.

\end{abstract}

\section{Introduction}

There are some ways to optimize computations and memory usage in the
word2vec implementation~\cite{1301.3781}. In this note we explore some of them.

\section{Implementation}

We've implemented the following word embedder using C programming language.
Source code can be found at \url{https://github.com/jelic98/raf_nlp}.

\subsection{Network architecture}

As in the word2vec implementation, we are using autoencoder
architecture which is capable of discovering structure within data in order
to develop a compressed representation of the input. If we pass word from
vocabulary to the input layer, then this representation will be word embedding.

\medbreak

Network consists of $3$ layers and $2$ weight matrices between them.

The first layer is the input layer which is onehot vector that uniquely represents a given word in the vocabulary.
We can denote it's vector as $\boldsymbol{x}$ and it's dimension as $I$.

\medbreak

The second layer $\boldsymbol{h}$ of dimension $J$ is the hidden layer which
input is the output from the previous
layer multiplied by the corresponding weight in a matrix $\boldsymbol{V}$.

\medbreak

The third layer $\boldsymbol{y}$ of dimension $K$ is the output layer which
input is the output from the previous
layer multiplied by the corresponding weight in a matrix $\boldsymbol{W}$.

\tikzset{%
	every neuron/.style={
		circle,
		draw,
		minimum size=1.0cm
	},
	neuron missing/.style={
		draw=none,
		scale=2,
		text height=0.25cm,
		execute at begin node=\color{black}$\vdots$
  }
}

\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}[x=1.5cm,y=1.5cm]
	\foreach \m/\l [count=\y] in {1,2,missing,3}
		\node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2-\y) {};
	\foreach \m [count=\y] in {1,missing,2}
		\node [every neuron/.try, neuron \m/.try] (hidden-\m) at (2,1.5-\y) {};
	\foreach \m/\l [count=\y] in {1,2,missing,3}
		\node [every neuron/.try, neuron \m/.try] (output-\m) at (4,2-\y) {};
	\foreach \l [count=\i] in {1,2,I}
		\draw [<-] (input-\i) -- ++(-1,0)
			node[above,xshift=1.5cm,yshift=-0.25cm] {$x_\l$};
	\foreach \l [count=\i] in {1,J}
		\node [above,yshift=-0.75cm] at (hidden-\i.north) {$h_\l$};
	\foreach \l [count=\i] in {1,2,K}
		\draw [->] (output-\i) -- ++(1,0)
			node[above,xshift=-1.5cm,yshift=-0.25cm] {$y_\l$};
	\foreach \a [count=\i] in {1,2,I}
		\foreach \b [count=\j] in {1,J}
			\draw [->] (input-\i) --
			(hidden-\j) node[midway,sloped,left,scale=0.75,yshift=0.25cm]
			{$V_{\a\b}$};
	\foreach \a [count=\i] in {1,J}
		\foreach \b [count=\j] in {1,2,K}
			\draw [->] (hidden-\i) --
			(output-\j) node[midway,sloped,left,scale=0.75,xshift=1cm,yshift=0.25cm]
			{$W_{\a\b}$};
	\node [align=center,above] at (0,1.5) {Input};
	\node [align=center,above] at (2,1.5) {Hidden};
	\node [align=center,above] at (4,1.5) {Output};
\end{tikzpicture}
\caption{\label{fig:caption} Network layers}
\end{center}
\end{figure}

\subsection{Vocabulary architecture}

Vocabulary is stored in memory as BST. Each node has the following

\begin{itemize}
	\item word string
	\item vocabulary index
	\item number of occurences in a corpus file
	\item size of context BoW
	\item context BoW as linked list
	\item auxiliary pointers
\end{itemize}

\subsection{Weights initialization}

For now, we are using just a randomly sampled real numbers from uniform distrubution in the interval
$[0, WEIGHT\_MAX]$. Prediction accuracy could potentially increase if we sample initial
weight values from Gaussian distrubution.

\subsection{Parsing pipeline}

Parsing pipeline overview is as follows

\begin{algorithm}[H]
	\caption{Parsing pipeline}
	\SetAlgoStyle
	$vocab \gets$ BinarySearchTree()\;
	\For{$word$ \textbf{in} $vocab$}{
		clean($word$)\;
		\If{$word \notin stopWords$}{
			$center \gets$ createNode($word$)\;
			insertBST($vocab$, $center$)\;
			$window[C] \gets center$\;
			\For{$c \gets 0$ \textbf{to} $C-1$}{
				\If{$window[c] \neq center$}{
					insertList($center.context$, $window[c]$)\;
					insertList($window[c].context$, $center$)\;
				}
				$window[c] = window[c+1]$\;
			}
		}
	}
\end{algorithm}

Firstly, we are reading a corpus file, i.e. unfiltered file with raw
text. While we are doing that, we are cleaning the corpus by removing special
characters, numbers and stop words, and converting every word to it's
lowercase representation and, optionally, stemming it. As we said, unique words are stored in BST
vocabuary so we are going to perferm recursive, but preferably iterative,
insert operation. We are using window sliding technique to create context BoW 
for each center word by appending pointers to a linked list contained in the
corresponding BST node.

\medbreak

After reading the corpus file and populating BST with unique words, we need to
dynamically allocate required collection of structures in memory. Those
collections include

\begin{itemize}
	\item onehot word representation array
	\item input, hidden and output network layer
	\item input$\rightarrow$hidden and hidden$\rightarrow$output weight matrices
	\item prediction errors array
	\item training order array
	\item helper sampling matrix for NS phase
\end{itemize}

\medbreak

Then, we are populating hash map which is going to be used for asscociating
word and it's unique vocabulary index generated by preorder BST traversal.

\medbreak

After that, we need to calculated word frequency that will be needed
int the NS stage for random word sampling using Monte Carlo method inverse
proportional to it's frequency in order to lower the chances of sampling
high-frquency words.

\subsection{Training pipeline}

Training phase consists of multiple epochs and in each one of them passing
every vocabulary word to input layer. We are not calculating loss function
because it's value is not required. Notice that we are done reading the file
and training phase is using only generated vocabulary from previous phase.
This results in training the network for a particular word only once and
independent of it's frequency.

\medbreak

To illustrate the idea mentioned above, let's compare word2vec and our implementation.

\textbf{word2vec implementation.}
Initialize sliding window at the beginning of the file. For each center word $\boldsymbol{x}$, create context BoW $\boldsymbol{C}$. For each context word $\boldsymbol{c} \in \boldsymbol{C}$, run training for pair $(\boldsymbol{x}, \boldsymbol{c})$ and calculate error vector $\boldsymbol{e}$ at the output layer. This way we run $|\boldsymbol{C}|$ training passes for each center word.

\textbf{Our implementation.}
Given a vocabulary $\mathcal{V}$, for each $\boldsymbol{x} \in \mathcal{V}$ run training for pair $(\boldsymbol{x}, \boldsymbol{C})$, where $\boldsymbol{C}$ is a context BoW of center word $\boldsymbol{x}$. This way we run only one training pass for each center word.

Training pipeline overview is as follows

\begin{algorithm}[H]
	\caption{Training pipeline}
	\SetAlgoStyle
	initializeWeights()\;
	\For{$epoch \gets 0$ \textbf{to} $EPOCH\_MAX$}{
		$vocab \gets$ shuffle($vocab$)\;
		\For{$word$ \textbf{in} $vocab$}{
			propagateForward()\;
			sampleNegatives()\;
			normalizeOutput()\;
			calculateLoss()\;
			propagateBackward()\;
		}
		serializeWeights()\;
	}
\end{algorithm}

\subsection{Testing pipeline}

Testing pipeline overview is as follows

\begin{algorithm}[H]
	\caption{Testing pipeline}
	\SetAlgoStyle
	$success \gets 0$\;
	$total \gets 0$\;
	deserializeWeights()\;
	\For{$word$ \textbf{in} $test$}{
		clean($word$)\;
		\If{$word \notin stopWords$}{
			propagateForward()\;
			normalizeOutput()\;
			$pred \gets$ argmax($output$)\;
			\If{$word = pred$}{
				$success \gets success + 1$\;
			}
			$total \gets total + 1$\;
		}
	}
	$acc \gets success / total$\;
\end{algorithm}

\subsection{Process of learning}

As defined in~\cite{1411.2738}, forawrd propagation of input is done as follows

\begin{align}
	&h_j = \sum_{i=1}^I (x_i \cdot V_{ij}) \\
	&y_k = \sum_{j=1}^J (h_j \cdot W_{jk})
\end{align}

where $k^\prime$ is and index of $c^{th}$ context word. Also, as stated
in~\cite{1512.05287} it's a good idea to dropout some neurons during a forward
propagation phase to avoid overfitting. We are using dropout in the output layer.

\begin{algorithm}[H]
	\caption{Forward propagation of input}
	\SetAlgoStyle
	\Fn{propagateForward()}{
		\For{$j \gets 0$ \textbf{to} $J$}{
			$h[j] \gets 0$\;
			\For{$i \gets 0$ \textbf{to} $I$}{
				$h[j] \gets h[j] + x[i] \cdot V[i, j]$\;
			}
		}
		\For{$k \gets 0$ \textbf{to} $K$}{
			$y[k] \gets 0$\;
			$p \gets$ random($0$, $1$)\;
			\If{$p < DROPOUT\_RATE$}{
				\textbf{continue}\;
			}
			\For{$j \gets 0$ \textbf{to} $J$}{
				$y[k] \gets y[k] + h[j] \cdot W[j, k]$\;
			}
		}
	}
\end{algorithm}

Loss calculation is as follows

\begin{align}
	\begin{aligned}
		L &= -\log P(x_{c_1}, x_{c_2},\ldots,x_{c_C}|x_0) \\
		&= -\log \prod_{c=1}^C P(x_{c_i}|x_0) \\
		&= -\log \prod_{c=1}^C softmax(\boldsymbol{y_c}) \\
		&= -\log \prod_{c=1}^C \frac{e^{y_{c_{k^\prime}}}}{\sum_{k=1}^K
		e^{y_{c_k}}} \\
		&= \sum_{c=1}^C \log\sum_{k=1}^K e^{y_{c_k}} - \sum_{c=1}^C
		y_{c_{k^\prime}}
	\end{aligned}
\end{align}

\begin{algorithm}[H]
	\caption{Loss calculation}
	\SetAlgoStyle
	\Fn{calculateLoss()}{
		\For{$k \gets 0$ \textbf{to} $K$}{
			$l[k] \gets 0$\;
			\For{$c$ \textbf{in} $context$}{
				$l[k] \gets l[k] + output[k]$\;
				\If{$k = indexOf(c)$}{
					$l[k] \gets l[k] - 1$\;
				}
			}
		}
	}
\end{algorithm}

Backward error propagation is done using relationships defined by

\begin{align}
	&W_{jk}^\prime = W_{jk} - \Delta W_{jk} =
	W_{jk} - \alpha \cdot l_k \cdot h_j \\
	&V_{ij}^\prime = V_{ij} - \Delta V_{ij} =
	V_{ij} - \alpha \cdot \sum_{k=1}^K (l_k \cdot W_{jk} \cdot x_i)
\end{align}

where $\alpha$ is a learning rate hyperparameter which is decreasing as
training progresses using simulated annealing technique.
Partial derrivatives we've used for applying delta rule are as follows

\begin{align}
	&\begin{aligned}
		\Delta W_{jk} &= \alpha \cdot \frac{\partial L}{\partial W_{jk}} \\
		&= \alpha \cdot \sum_{k=1}^K \sum_{c=1}^C \frac{\partial L}{\partial y_{c_k}}
		\cdot \frac{\partial y_{c_k}}{\partial W_{jk}} \\
		&= \alpha \cdot \sum_{k=1}^K \sum_{c=1}^C l_{c_k} \cdot h_j \\
		&= \alpha \cdot l_k \cdot h_j
	\end{aligned} \\
	&\begin{aligned}
		\Delta V_{ij} &= \alpha \cdot \frac{\partial L}{\partial V_{ij}} \\
		&= \alpha \cdot \sum_{k=1}^K \sum_{c=1}^C \frac{\partial L}{\partial y_{c_k}}
		\cdot \frac{\partial y_{c_k}}{\partial h_j} \cdot
		\frac{\partial h_j}{\partial V_{ij}} \\
		&= \alpha \cdot \sum_{k=1}^K \sum_{c=1}^C l_{c_k} \cdot W_{jk} \cdot x_i \\
		&= \alpha \cdot \sum_{k=1}^K (l_k \cdot W_{jk} \cdot x_i)
	\end{aligned}
\end{align}

where $l_{c_k}$ represents a loss for $c^{th}$ context word on $k^{th}$ output layer
neuron and it's calculated by

\begin{align}
	&l_{c_k} = y_{c_k} - t_{c_k} \\
	&t_{c_k} =
	\begin{cases}
		1, &\text{if } \boldsymbol{c_k} \in \boldsymbol{C} \\
		0, &\text{otherwise}
	\end{cases}
\end{align}

Can we use Adam optimizer to speed up gradient descent?
Also, how can we implement cross-validation?

\begin{algorithm}[H]
	\caption{Backward propagation of error}
	\SetAlgoStyle
	\Fn{propagateBackward()}{
		$i \gets$ indexOf($word$)\;
		\For{$j \gets 0$ \textbf{to} $J$}{
			$lw \gets 0$\;
			\For{$k \gets 0$ \textbf{to} $K$}{
				$W[j, k] \gets W[j, k] - \alpha \cdot l[k] \cdot h[j]$\;
				$lw \gets lw + l[k] \cdot W[j, k]$\;
			}
			$V[i, j] \gets V[i, j] - \alpha \cdot lw$\;
		}
	}
\end{algorithm}

Notice that we are ignoring $x_i$ factor in $lw$ calculation because $x_i
\neq 0$ only for $i^{th}$ center word. This way we only update
input$\rightarrow$hidden weights only for the given center word.

\subsection{Hyperparameters}

We are using hyperparameters defined below to tweak our network in order to get the best accuracy and performance.

\begin{itemize}
	\item $J$ - Number of neurons in the hidden layer
	\item $C$ - Size of sliding window, i.e. size of context BoW
	\item $EPOCH\_MAX$ - Total epochs per training
	\item $WEIGHT\_MAX$ - Upper bound for initial weight value sampling
	\item $ALPHA\_MAX$ - Initial learning rate value
	\item $ALPHA\_MIN$ - Final learning rate value
	\item $DROPOUT\_RATE$ - Probability of neuron dropout
	\item $SAMPLE\_MAX$ - Number of negative samples to pick
	\item $SAMPLE\_FACTOR$ - Normalized frequency averaged across all words
	\item $SAMPLE\_EXIT$ - Upper bound for unsuccessful samplings
\end{itemize}

\medbreak

Value for $J$ hyperparameter would preferably be selected using an algorithm proposed by Branislav MilojkoviÄ‡.

\section{Optimization}

\subsection{Negative sampling}

Negative sampling is a special case of Noise Contrastive Estimation (NCE)~\cite{1206.6426}.
Implementation proposed by~\cite{1704.03956} is in progress.

\subsection{Multithreading}

Implementation is in progress.

\section{Vector representations of sentences}

Word embeddings on their own are useful for analyzing relationships between
words, but encoding sentences can give us better semantic understanding of
larger chunks of text. Sentence encoding is done as follows

\begin{algorithm}[H]
	\caption{Sentence encoding}
	\SetAlgoStyle
	\Fn{encode(sentence)}{
		\For{$word$ \textbf{in} $sentence$}{
			clean($word$)\;
			\If{$word \notin stopWords$}{
				$i \gets$ indexOf($word$)\;
				\For{$j \gets 0$ \textbf{to} $J$}{
					$vec[j] \gets vec[j] + V[i][j]$\;
				}
			}
		}
		\Return{$vec$}\;
	}
\end{algorithm}

\section{Fast tests}

What if we can predict next word just by running statistical test on our vocabulary or
sampling context words (predictions) using Gaussian distrubution?

\begin{align}	
	&\begin{aligned}	
		E &= -\log P(x_{c_1}, x_{c_2},\ldots,x_{c_C}|x_0) \\
		&= -\log \prod_{c=1}^C P(x_{c_i}|x_0) \\
		&= -\log \prod_{c=1}^C \frac{P(x_{c_i}x_0)}{P(x_0)} \\
		&= -\log \prod_{c=1}^C \frac{\phi_{x_0}(x_{c_i})}{\phi(x_0)}
	\end{aligned}
\end{align}

where $\phi(x_i)$ denotes non-normalized (arithmetic underflow problem)
frequency of a word $x_i$ and $\phi_{x_0}(x_{c_i})$ denotes frequency of
a word $x_{c_i}$ in context BoW of a word $x_0$

What if we use some rules to score each word in a context BoW? Can we agree
on the following hypothesis?

\medbreak

\textit{Context word just before a given center word has a
higher impact on prediction than a word which is $n$ places after a given
center word.}

\section{Results and discussion}

Run embedder on a large dataset and generate accuracy and performance report.

\fig{chart}{Very useful dot}

\bibliographystyle{plain}
\bibliography{\jobname}

\end{document}
